<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="RL," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="本文翻译自David Silver的《Tutorial: Deep Reinforcement Learning》 这是一篇阅读笔记，如有写错，请联系susht3@foxmail.com，一起讨论。 本文结构一. 前言 二. 深度学习简介 三. 强化学习简介 四. 基于value的强化学习 五. 基于policy的强化学习 六. 基于model的强化学习 一. 前言深度学习 vs 强化学习深度学习">
<meta name="keywords" content="RL">
<meta property="og:type" content="article">
<meta property="og:title" content="《Deep_RL_Tutorial》之入门强化学习">
<meta property="og:url" content="http://yoursite.com/2017/09/23/Deep-RL-Tutorial/index.html">
<meta property="og:site_name" content="susht">
<meta property="og:description" content="本文翻译自David Silver的《Tutorial: Deep Reinforcement Learning》 这是一篇阅读笔记，如有写错，请联系susht3@foxmail.com，一起讨论。 本文结构一. 前言 二. 深度学习简介 三. 强化学习简介 四. 基于value的强化学习 五. 基于policy的强化学习 六. 基于model的强化学习 一. 前言深度学习 vs 强化学习深度学习">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/deep.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/back.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/linear.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/non-linear.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/loss.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/der.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/w.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/moutain.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/two.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/rnn.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/cnn.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/rl.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/agent.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/state.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/policy.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/value.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/update.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/bell.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/optim.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/bell2.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/model.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/dqn.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/q-learn.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/experience.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/target.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/atari.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/doubleQ.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/replay.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/duelling.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/policy2.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/object.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/gradient.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/critic.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/q.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/actor.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/critic2.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/lab.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/critic3.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/actor2.png?raw=true">
<meta property="og:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/sim.png?raw=true">
<meta property="og:updated_time" content="2017-09-23T14:36:04.414Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="《Deep_RL_Tutorial》之入门强化学习">
<meta name="twitter:description" content="本文翻译自David Silver的《Tutorial: Deep Reinforcement Learning》 这是一篇阅读笔记，如有写错，请联系susht3@foxmail.com，一起讨论。 本文结构一. 前言 二. 深度学习简介 三. 强化学习简介 四. 基于value的强化学习 五. 基于policy的强化学习 六. 基于model的强化学习 一. 前言深度学习 vs 强化学习深度学习">
<meta name="twitter:image" content="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/deep.png?raw=true">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/09/23/Deep-RL-Tutorial/"/>





  <title>《Deep_RL_Tutorial》之入门强化学习 | susht</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">susht</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/23/Deep-RL-Tutorial/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="susht">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="susht">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">《Deep_RL_Tutorial》之入门强化学习</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-23T19:19:03+08:00">
                2017-09-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文翻译自David Silver的《Tutorial: Deep Reinforcement Learning》</p>
<p>这是一篇阅读笔记，如有写错，请联系susht3@foxmail.com，一起讨论。</p>
<h4 id="本文结构"><a href="#本文结构" class="headerlink" title="本文结构"></a>本文结构</h4><p>一. 前言</p>
<p>二. 深度学习简介</p>
<p>三. 强化学习简介</p>
<p>四. 基于value的强化学习</p>
<p>五. 基于policy的强化学习</p>
<p>六. 基于model的强化学习</p>
<h2 id="一-前言"><a href="#一-前言" class="headerlink" title="一. 前言"></a>一. 前言</h2><h3 id="深度学习-vs-强化学习"><a href="#深度学习-vs-强化学习" class="headerlink" title="深度学习 vs 强化学习"></a>深度学习 vs 强化学习</h3><p>深度学习是特征学习，给定一个目标，学习目标的特征，使用最原始的特征输入，利用最少的领域知识。</p>
<p>强化学习是决策学习，针对具有选择action能力的agent，每个action都会影响到agent未来的状态state，利用一个标量的奖励reward来衡量成功度，目标是选择一系列的action以最大化未来的奖励reward。</p>
<p>深度强化学习结合DL+RL，RL定义目标，DL给出计算过程。</p>
<h2 id="二-深度学习简介"><a href="#二-深度学习简介" class="headerlink" title="二. 深度学习简介"></a>二. 深度学习简介</h2><p>深度学习是用来实现机器学习的一种方法！深度神经网络用于理解深度表征。</p>
<h3 id="1-深度表征"><a href="#1-深度表征" class="headerlink" title="1. 深度表征"></a>1. 深度表征</h3><p>深度表征由很多函数组成：</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/deep.png?raw=true" style="zoom:60%"><br></div>



<p>通过后向传播来更新梯度：</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/back.png?raw=true" style="zoom:60%"><br></div>


<p>注意：网络过深会导致网络权重更新不稳定，会带来梯度爆炸或者梯度消失的问题，有时候需要进行梯度截断。梯度消失，层数越多，求导结果越小，所以越是靠前的网络层权重几乎不更新，只学习到接近最后输出层的几层网络。梯度爆炸，w比较大。还跟学习率有关。<br>梯度爆炸和梯度消失的本质原因：梯度反向传播中的连乘效应。</p>
<h3 id="2-深度神经网络"><a href="#2-深度神经网络" class="headerlink" title="2. 深度神经网络"></a>2. 深度神经网络</h3><p>线性变换：</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/linear.png?raw=true" style="zoom:50%"><br></div>

<p>非线性激活函数：</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/non-linear.png?raw=true" style="zoom:50%"><br></div>

<p>loss函数：</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/loss.png?raw=true" style="zoom:50%"><br></div>

<p>有多种loss函数，平方差，对数似然函数，交叉熵等等。</p>
<h3 id="3-随机梯度下降法"><a href="#3-随机梯度下降法" class="headerlink" title="3. 随机梯度下降法"></a>3. 随机梯度下降法</h3><p>对loss求导：</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/der.png?raw=true" style="zoom:50%"><br></div>


<p>根据梯度下降的方向去调整w：</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/w.png?raw=true" style="zoom:50%"><br></div>    


<p>梯度下降法用于求解一个函数J(θ)的最大值或者最小值，而一个函数沿着梯度下降的方向是下降最快的，导数为0的那一点就是最值点。<br>现在目标是更新θ使得J(θ)最小，先选个初始值，然后不断往梯度的方向更新θ，直到θ的值不再改变。更新公式中学习率决定下降步伐，梯度决定下降方向。学习率用于控制迭代速度，取值太小会导致迭代过慢，取值太大可能错过最值点。</p>
<h5 id="下山例子"><a href="#下山例子" class="headerlink" title="下山例子"></a>下山例子</h5><div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/moutain.png?raw=true" style="zoom:60%"><br></div>

<p>这里纵坐标表示目函数的值（即 J( θ) 的值），两个横坐标分别表示参数 θ1 和 θ2。从初始点开总是往下降速度最快的地方走 ，这个地方也就是该点的梯度。</p>
<h5 id="二维的等高线图"><a href="#二维的等高线图" class="headerlink" title="二维的等高线图"></a>二维的等高线图</h5><div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/two.png?raw=true" style="zoom:60%"><br></div>

<p>迭代特点：局部最优解<br>初始值有影响，初始值不同，得到的最小值也有不同；每一步都选择梯度下降的方向走，越是接近最小值，下降速度就越慢。</p>
<p>批量梯度下降：每次更新都遍历m个训练样本。缺点是训练集很大的时候，耗时太长。</p>
<p>随机梯度下降：依次使用第i个样本进行更新。缺点是它可能在全局最优解的附近徘徊，但是总体趋向全局最小值。</p>
<h3 id="4-权重共享"><a href="#4-权重共享" class="headerlink" title="4. 权重共享"></a>4. 权重共享</h3><p>这是减少参数数量的关键！</p>
<h4 id="循环神经网络-RNN"><a href="#循环神经网络-RNN" class="headerlink" title="循环神经网络 RNN"></a>循环神经网络 RNN</h4><p>在时间步长上共享参数</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/rnn.png?raw=true" style="zoom:60%"><br></div>


<h4 id="卷积神经网络-CNN"><a href="#卷积神经网络-CNN" class="headerlink" title="卷积神经网络 CNN"></a>卷积神经网络 CNN</h4><p>在空间区域中共享参数。</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/cnn.png?raw=true" style="zoom:60%"><br></div>

<p>局部连接+权重共享。局部连接，每个隐藏层节点每次都只连接到一个局部区域。权重共享，卷积核的权重参数共享到所有的神经元。</p>
<h2 id="三-强化学习简介"><a href="#三-强化学习简介" class="headerlink" title="三. 强化学习简介"></a>三. 强化学习简介</h2><h3 id="1-RL的地位"><a href="#1-RL的地位" class="headerlink" title="1. RL的地位"></a>1. RL的地位</h3><p>无处不在的RL，作为许多领域的中心思想，优化决策以获得最佳结果。RL是一个序列决策问题，选择一系列的决策，使得最终收益最高。</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/rl.png?raw=true" style="zoom:60%"><br></div>

<p>（下面这段话引用自chenrudan）</p>
<p>强化学习是多学科多领域交叉的一个产物，它的本质就是解决“decision making”问题，即学会自动进行决策。在computer science领域体现为机器学习算法。在Engineering领域体现在决定the sequence of actions来得到最好的结果。在Neuroscience领域体现在理解人类大脑如何做出决策，主要的研究是reward system。在Psychology领域，研究动物如何做出决策，动物的行为是由什么导致的。在Economics领域体现在博弈论的研究。</p>
<h3 id="2-基本概念"><a href="#2-基本概念" class="headerlink" title="2.  基本概念"></a>2.  基本概念</h3><h4 id="Agent-和-Environment"><a href="#Agent-和-Environment" class="headerlink" title="Agent 和 Environment"></a>Agent 和 Environment</h4><div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/agent.png?raw=true" style="zoom:60%"><br></div>

<p>在每个时刻下：</p>
<p>Agent:执行某个action at，然后接收来自环境的观察值ot与瞬时奖励rt</p>
<p>Environment:接收来自agent 的一个动作action，发出观察信号o(t+1)，和奖励信号r(t+1)</p>
<p>(confuse：这里我觉得应该是ot和rt，也有可能是因为延时？)</p>
<h4 id="State"><a href="#State" class="headerlink" title="State"></a>State</h4><div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/state.png?raw=true" style="zoom:60%"><br></div>

<p>Experience(历史状态):包含一系列的从第一个时刻开始的observations, actions, rewards</p>
<p>State(状态):<br>state是experience的总和。在某个时刻t下，状态state st是这个时刻以及之前所有时刻的所有历史状态的函数。其实严格意义上有两种state：environment state是环境自身的状态，agent state是环境反馈给agent的状态，也就是平常简称的state。环境会通过observation来告诉agent当前的环境状态。</p>
<p>观测性:在一个全观测的环境中，agent state等同于environment state。在某个时刻t下，state是当前时刻下观测值ot的一个函数，这里具备马尔科夫性质。在一个部分可观测的环境中，那么agent获取到的就不是最直接的environment state。</p>
<h3 id="3-Agent的组成"><a href="#3-Agent的组成" class="headerlink" title="3. Agent的组成"></a>3. Agent的组成</h3><p>一个Agent由policy,value function和model这三部分组成，但是这三部分不是必须要存在的。</p>
<h4 id="Policy"><a href="#Policy" class="headerlink" title="Policy"></a>Policy</h4><p>policy是agent的决策行为，根据state来选择action，函数形式是将state映射为action，包含两种表达方式。确定性策略根据state直接选择某个action。随机性策略根据state生成action的概率分布，再从中选择action。</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/policy.png?raw=true" style="zoom:60%"><br></div>

<h4 id="Value-function"><a href="#Value-function" class="headerlink" title="Value function"></a>Value function</h4><p>value函数预测当前state s下，未来可能获得的reward的期望，用来衡量当前state的好坏。Vπ(s)=Eπ[Rt+1+rRt+2+…|St=s]，已有policy去选择action。而Q函数是奖励总值的期望，给定state s，action a，policy π，以及衰减因子γ，计算综合奖励的期望。Q用来衡量当前state下选择某个action的好坏。</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/value.png?raw=true" style="zoom:60%"><br></div>

<p>注意！V和Q的区别在于，v是state value，Q是state-action value。Q里面的action a，不是用这个policy去算当前的action，而是当前state s已经采取了这个action a，后面的state s’再利用这个policy去选择后面的action a’，得到后面的reward。</p>
<h5 id="如何更新value："><a href="#如何更新value：" class="headerlink" title="如何更新value："></a>如何更新value：</h5><div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/update.png?raw=true" style="zoom:60%"><br></div>

<p>v可以采取某个policy选择一个action，或者遍历所有action选择v最大的。公式左边是即时奖励，右边是带有衰减因子的未来奖励。</p>
<h5 id="贝尔曼方程"><a href="#贝尔曼方程" class="headerlink" title="贝尔曼方程"></a>贝尔曼方程</h5><p>Q函数还可以分解为贝尔曼方程：</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/bell.png?raw=true" style="zoom:60%"><br></div>

<p>后面的r用Q(s’)来表示，s’为当前state s的下一个状态，a’为下一个action<br>Q函数是一个期望！关于后面state和action的一个期望。</p>
<h5 id="优化价值函数"><a href="#优化价值函数" class="headerlink" title="优化价值函数"></a>优化价值函数</h5><p>就是使得价值函数达到最大值。给定state s和action a，计算在所有policy下，当前state和action的Q函数，而最大的Q就是当前state和action下最优的Q函数。也就是说，在当前state下，每个action都有一个最优的Q函数。然后遍历所有action，在这些最优Q当中，再次选择一个最大Q，选择这个最大Q的action，以此作为当前state下的policy π。</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/optim.png?raw=true" style="zoom:60%"><br></div>

<p>注意：最优Q是带有星号的Q<em>，Q</em>其实就是在Q中挑选出最优的那个。</p>
<p>最优价值函数也可以分解为贝尔曼方程：</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/bell2.png?raw=true" style="zoom:60%"><br></div>

<p>Q和Q<em>的贝尔曼方程区别在于，Q是对s’和a’的期望，<br>Q</em>是对s’的期望，因为Q*会选择使得Q(s’，a’)最大的action a’</p>
<h4 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h4><p>Model从经历中学习，学习环境的工作方式，预测下一个state或者下一次reward。</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/model.png?raw=true" style="zoom:60%"><br></div>


<h3 id="4-实现强化学习的方法"><a href="#4-实现强化学习的方法" class="headerlink" title="4. 实现强化学习的方法"></a>4. 实现强化学习的方法</h3><p>前面提到agent的三个组成部分，可以根据这三部分去实现强化学习。</p>
<p>基于value的方法估算Q函数的最大值Q*(s, a)，注意：这是基于任意policy下都能得到的最优Q值！policy π的参数会变，所以会有不同的π，那么不同的π就会得到不同的action，其实 Under any policy 最后就是会遍历到所有的action。</p>
<p>基于policy的方法寻找最优policy π*，在这个policy下能够得到最大的未来奖励。</p>
<p>基于model的方法对环境进行建模，用这个模型来进行规划，比如前向搜索。</p>
<h4 id="DeepMind-DRL例子"><a href="#DeepMind-DRL例子" class="headerlink" title="DeepMind-DRL例子"></a>DeepMind-DRL例子</h4><p>游戏：Atari 游戏、扑克、围棋</p>
<p>探索世界：3D 世界、迷宫</p>
<p>控制物理系统：操作、步行、游泳</p>
<p>与用户互动：推荐、优化、个性化</p>
<p>实现方法：使用深度学习去模拟Q，policy，model，再用随机梯度下降法去优化loss函数。</p>
<h2 id="四-基于value的深度学习"><a href="#四-基于value的深度学习" class="headerlink" title="四. 基于value的深度学习"></a>四. 基于value的深度学习</h2><h3 id="1-Q网络"><a href="#1-Q网络" class="headerlink" title="1. Q网络"></a>1. Q网络</h3><p>用神经网络对Q进行建模：</p>
<p>第一类做法： 输入s与a，输出一个Q(s，a，w)值，其中w为网络权重</p>
<p>第二类做法： 输入s，输出一个Q(s，ai，w)的向量，维度为action的维度大小</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/dqn.png?raw=true" style="zoom:60%"><br></div>



<h3 id="2-Q-Learning算法"><a href="#2-Q-Learning算法" class="headerlink" title="2. Q-Learning算法"></a>2. Q-Learning算法</h3><h4 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h4><p>把最优的Q*定义为我们要求的目标，用随机梯度下降法去最小化均方差MSE</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/q-learn.png?raw=true" style="zoom:60%"><br></div>


<h4 id="神经网络所产生的问题"><a href="#神经网络所产生的问题" class="headerlink" title="神经网络所产生的问题"></a>神经网络所产生的问题</h4><p>采样之间的相关性：当前的state和action和上一步执行的action非常接近！数据集具备序列性质，具有很强的相关性。</p>
<p>目标的不稳定性：注意 我们学习的目标是依赖于目标本身的。不平稳性会导致近似函数螺旋式失控，policy会有震荡。</p>
<p>为了解决这些问题，接下来会介绍DQN算法。</p>
<h3 id="3-DQN算法"><a href="#3-DQN算法" class="headerlink" title="3. DQN算法"></a>3. DQN算法</h3><h4 id="经验回放"><a href="#经验回放" class="headerlink" title="经验回放"></a>经验回放</h4><p>从过往的经历构建出一个数据集(s，a，r’，s’)，从这个数据集中随机采样一些经历(s，a，r，s’)进行更新。</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/experience.png?raw=true" style="zoom:60%"><br></div>

<p>这些采样出来的经历是独立分布的，这样就可以打破训练集之间的相关性。</p>
<p>注意！构建数据集的时候使用(s，a，r’，s’)，但是采样的时候使用(s，a，r，s’)。为啥？还是因为 延迟性？？mark</p>
<h4 id="固定target-Q网络"><a href="#固定target-Q网络" class="headerlink" title="固定target-Q网络"></a>固定target-Q网络</h4><p>公式左边是target Q，右边是我们训练的Q网络。使用MSE损失函数。</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/target.png?raw=true" style="zoom:60%"><br></div>

<p>这里暂时hold住Q网络以前的权重w-，用这个固定住的参数w-去算target Q，然后周期性更新W-为w。而Q网络仍是你Q网络，Q网络是时刻更新的，只是target Q会暂时固定住的。这样可以减少震荡。</p>
<h4 id="剪切回报-值范围"><a href="#剪切回报-值范围" class="headerlink" title="剪切回报/值范围"></a>剪切回报/值范围</h4><p>设置reward在[-1，+1]范围内，可以保证Q的值不会太大，好求梯度。</p>
<h4 id="DQN玩Atari游戏"><a href="#DQN玩Atari游戏" class="headerlink" title="DQN玩Atari游戏"></a>DQN玩Atari游戏</h4><p>从像素s中端到端学习Q（s，a），输入是前面4帧的像素，作为state，输出是对应按钮位置的Q（s，a），那么reward就是执行这个action后，得分和从前的差值了。</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/atari.png?raw=true" style="zoom:60%"><br></div>    



<p>接下来会介绍一些基于DQN的改进方法。</p>
<h4 id="4-Double-DQN"><a href="#4-Double-DQN" class="headerlink" title="4. Double-DQN"></a>4. Double-DQN</h4><p>用两个网络，Q(w-)用于选择action，Q(w)用于评估action</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/doubleQ.png?raw=true" style="zoom:60%"><br></div>    


<p>Double-DQN与DQN的区别在于，在下一个state s’中，DQN遍历a’使用max Q作为target，这里评估action的Q是暂时固定住的Q，这是会有偏差的。Double-DQN同样也是先选出max Q对应的action a’，然后再用网络Q(w-)去评估这个action，以Q(a’，w-)的结果作为target Q。</p>
<h4 id="5-确定优先级的经验回放"><a href="#5-确定优先级的经验回放" class="headerlink" title="5.确定优先级的经验回放"></a>5.确定优先级的经验回放</h4><p>从前做经验回放的时候，所有经历都是等权重的。这里以DQN的误差去衡量经历的优先级，越是做得差的经历越是要回放。</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/replay.png?raw=true" style="zoom:60%"><br></div>    


<h4 id="6-决斗网络"><a href="#6-决斗网络" class="headerlink" title="6. 决斗网络"></a>6. 决斗网络</h4><p>将Q网络分成两个channel，Action-indepent用于计算当忽略了某些动作时 你会得到多少奖励。在V(s，v)中，小v是V网络的参数，s是state。Action-dependent用于计算当你采取了某个动作a时 你会得到多少好处。最后结合这两部分，总和作为Q。</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/duelling.png?raw=true" style="zoom:60%"><br></div>    



<h4 id="加速训练"><a href="#加速训练" class="headerlink" title="加速训练"></a>加速训练</h4><p>Gorila结构，使用不同机器，并行化运行Q网络。</p>
<p>如果你没有google的资源，还可以使用异步强化学习：利用标准 CPU 的多线程；并行执行agent的多个实例；线程间共享网络参数；并行消除数据相关性；替代经历回放；在单个机器上模仿Gorila的加速过程。</p>
<h2 id="五-基于policy的强化学习"><a href="#五-基于policy的强化学习" class="headerlink" title="五. 基于policy的强化学习"></a>五. 基于policy的强化学习</h2><h3 id="1-Deep-Policy-Networks"><a href="#1-Deep-Policy-Networks" class="headerlink" title="1. Deep Policy Networks"></a>1. Deep Policy Networks</h3><h4 id="策略方式"><a href="#策略方式" class="headerlink" title="策略方式"></a>策略方式</h4><p>随机性policy：根据state生成action的概率分布，表示在某个state下执行某个action的概率。随机性policy比确定性policy要好。</p>
<p>确定性policy：根据state直接生成action</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/policy2.png?raw=true" style="zoom:60%"><br></div>        

<h4 id="目标函数-1"><a href="#目标函数-1" class="headerlink" title="目标函数"></a>目标函数</h4><p>将目标函数定义为 总reward，使用随机梯度下降法来最大化目标函数。</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/object.png?raw=true" style="zoom:60%"><br></div>    

<p>这个写法感觉有点怪怪的，其中u是网络的参数，π是policy，点号不知道是什么。可以写成，在这个policy下，所有action对应的的总reward的期望。目标形式依然是优化Q，但是和前面不同的是，这里Q的参数是policy的参数。</p>
<h4 id="Policy-Gradients"><a href="#Policy-Gradients" class="headerlink" title="Policy Gradients"></a>Policy Gradients</h4><p>目标函数分别对随机性policy和确定性policy求梯度：</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/gradient.png?raw=true" style="zoom:60%"><br></div>        

<p>对于随机性policy，把L(u)写成E的形式再求导，就可以得到图中结果。公式中第一项是score function，用来衡量policy选择这个action的程度，第二项是Q，用来衡量这个action的好坏。如果这个action表现较好，我们就调整policy趋近这个action，最终拉近这两个分布。</p>
<p>对于确定性policy，链式求导。</p>
<h3 id="2-Actor-Critic算法"><a href="#2-Actor-Critic算法" class="headerlink" title="2.  Actor-Critic算法"></a>2.  Actor-Critic算法</h3><p>Actor-Critic算法包含两个网络，Actor 网络用于选择action，Critic 网络用于评估这个action的好坏程度，计算value function，去指导Actor网络更新梯度。Actor基于policy gradients，可以在连续的actio空间选择一个action，而critic基于Q-learning，可以进行单步更新，注意policy gradient是回合更新，会降低学习效率。所以Actor-Critic是结合了基于policy和基于value的方法。</p>
<h4 id="Actor-网络"><a href="#Actor-网络" class="headerlink" title="Actor 网络"></a>Actor 网络</h4><p>Actor网络用于学习policy，输入state，输出action</p>
<h3 id="Critic-网络"><a href="#Critic-网络" class="headerlink" title="Critic 网络"></a>Critic 网络</h3><p>Critic网络用于学习Q，以评估action的好坏，输入state和action，输出对应的Q，Critic的目标就是估计这个policy的贝尔曼方程。</p>
<p>那么接下来actor就可以用这个Q去更新自己的梯度，往提高Q的方向去更新。</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/critic.png?raw=true" style="zoom:60%"><br></div>    

<p>这里w是critic的参数，u是actor的参数，这里是对u求导，以更新policy。</p>
<h3 id="3-Asynchronous-Advantage-Actor-Critic-A3C"><a href="#3-Asynchronous-Advantage-Actor-Critic-A3C" class="headerlink" title="3.  Asynchronous Advantage Actor-Critic (A3C)"></a>3.  Asynchronous Advantage Actor-Critic (A3C)</h3><p>V是state-value function，用于评估在当前state的好坏。<br>Q则是state-action value，这里和之前定义的Q所有不同，这里只有前n个reward是用reward来算，后面的reward总值使用V去评估。</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/q.png?raw=true" style="zoom:60%"><br></div>        

<h4 id="Actor-网络-1"><a href="#Actor-网络-1" class="headerlink" title="Actor 网络"></a>Actor 网络</h4><p>与先前actor不同的地方是，公式右边变成Q-V，这个就叫Advantage，表示采取这个action比平均state下要好出多少。</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/actor.png?raw=true" style="zoom:60%"><br></div>        

<h4 id="Critic-网络-1"><a href="#Critic-网络-1" class="headerlink" title="Critic 网络"></a>Critic 网络</h4><p>Critic网络评估的是V值，目标函数是 根据q 和V 的MSE函数。</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/critic2.png?raw=true" style="zoom:60%"><br></div>    

<p>咩咩咩？？？mark</p>
<h4 id="A3C用于Labyrinth游戏"><a href="#A3C用于Labyrinth游戏" class="headerlink" title="A3C用于Labyrinth游戏"></a>A3C用于Labyrinth游戏</h4><p>定义state为当前画面的像素，使用lstm对state进行建模，每个state输出action的概率分布，以及这个state的value值。</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/lab.png?raw=true" style="zoom:60%"><br></div>    


<h3 id="DRL如何处理高维连续action空间？"><a href="#DRL如何处理高维连续action空间？" class="headerlink" title="DRL如何处理高维连续action空间？"></a>DRL如何处理高维连续action空间？</h3><ol>
<li><p>如果对Q算一个max，计算量会非常大。</p>
<p> Actor-Critic算法不需要算max Q，避开了这个问题。</p>
</li>
<li><p>Q值是可微的。</p>
<p> 确定性的policy gradient会对Q求导。</p>
</li>
</ol>
<h3 id="4-Deep-DPG"><a href="#4-Deep-DPG" class="headerlink" title="4. Deep DPG"></a>4. Deep DPG</h3><p>DDPG同时借鉴了DQN和Actor-Critic，DDPG的优势在于连续action空间。</p>
<h4 id="经验回放-1"><a href="#经验回放-1" class="headerlink" title="经验回放"></a>经验回放</h4><p>前面提到的。</p>
<h4 id="Critic-网络-2"><a href="#Critic-网络-2" class="headerlink" title="Critic 网络"></a>Critic 网络</h4><p>DDPG和DQN不一样的地方在于，DDPG多了个policy π，π的参数为u-，target-Q中会暂时hold住u-和w-</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/critic3.png?raw=true" style="zoom:60%"><br></div>    


<h4 id="Actor-网络-2"><a href="#Actor-网络-2" class="headerlink" title="Actor 网络"></a>Actor 网络</h4><p>Actor网络更新的时候也是朝着提高Q的方向。</p>
<div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/actor2.png?raw=true" style="zoom:60%"><br></div>    


<h4 id="DDPG用于Simulated-Physics"><a href="#DDPG用于Simulated-Physics" class="headerlink" title="DDPG用于Simulated Physics"></a>DDPG用于Simulated Physics</h4><div align="center"><br><img src="https://github.com/susht3/pictures_for_blog/blob/master/deep_rl_tutorial/sim.png?raw=true" style="zoom:60%"><br></div>    


<h2 id="六-基于model的强化学习"><a href="#六-基于model的强化学习" class="headerlink" title="六. 基于model的强化学习"></a>六. 基于model的强化学习</h2><p>未完待续 暂时写不下去了….回宿舍</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/RL/" rel="tag"># RL</a>
          
        </div>
      

      
      
      

      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          
            <p class="site-author-name" itemprop="name">susht</p>
            <p class="site-description motion-element" itemprop="description"></p>
        </div>

        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">tags</span>
              
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#本文结构"><span class="nav-number">1.</span> <span class="nav-text">本文结构</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#一-前言"><span class="nav-number"></span> <span class="nav-text">一. 前言</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#深度学习-vs-强化学习"><span class="nav-number"></span> <span class="nav-text">深度学习 vs 强化学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二-深度学习简介"><span class="nav-number"></span> <span class="nav-text">二. 深度学习简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-深度表征"><span class="nav-number"></span> <span class="nav-text">1. 深度表征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-深度神经网络"><span class="nav-number"></span> <span class="nav-text">2. 深度神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-随机梯度下降法"><span class="nav-number"></span> <span class="nav-text">3. 随机梯度下降法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#下山例子"><span class="nav-number">0.1.</span> <span class="nav-text">下山例子</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#二维的等高线图"><span class="nav-number">0.2.</span> <span class="nav-text">二维的等高线图</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-权重共享"><span class="nav-number"></span> <span class="nav-text">4. 权重共享</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#循环神经网络-RNN"><span class="nav-number">1.</span> <span class="nav-text">循环神经网络 RNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积神经网络-CNN"><span class="nav-number">2.</span> <span class="nav-text">卷积神经网络 CNN</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#三-强化学习简介"><span class="nav-number"></span> <span class="nav-text">三. 强化学习简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-RL的地位"><span class="nav-number"></span> <span class="nav-text">1. RL的地位</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-基本概念"><span class="nav-number"></span> <span class="nav-text">2.  基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Agent-和-Environment"><span class="nav-number">1.</span> <span class="nav-text">Agent 和 Environment</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#State"><span class="nav-number">2.</span> <span class="nav-text">State</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Agent的组成"><span class="nav-number"></span> <span class="nav-text">3. Agent的组成</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Policy"><span class="nav-number">1.</span> <span class="nav-text">Policy</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Value-function"><span class="nav-number">2.</span> <span class="nav-text">Value function</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#如何更新value："><span class="nav-number">2.1.</span> <span class="nav-text">如何更新value：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#贝尔曼方程"><span class="nav-number">2.2.</span> <span class="nav-text">贝尔曼方程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#优化价值函数"><span class="nav-number">2.3.</span> <span class="nav-text">优化价值函数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Model"><span class="nav-number">3.</span> <span class="nav-text">Model</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-实现强化学习的方法"><span class="nav-number"></span> <span class="nav-text">4. 实现强化学习的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DeepMind-DRL例子"><span class="nav-number">1.</span> <span class="nav-text">DeepMind-DRL例子</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#四-基于value的深度学习"><span class="nav-number"></span> <span class="nav-text">四. 基于value的深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Q网络"><span class="nav-number"></span> <span class="nav-text">1. Q网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Q-Learning算法"><span class="nav-number"></span> <span class="nav-text">2. Q-Learning算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#目标函数"><span class="nav-number">1.</span> <span class="nav-text">目标函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#神经网络所产生的问题"><span class="nav-number">2.</span> <span class="nav-text">神经网络所产生的问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-DQN算法"><span class="nav-number"></span> <span class="nav-text">3. DQN算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#经验回放"><span class="nav-number">1.</span> <span class="nav-text">经验回放</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#固定target-Q网络"><span class="nav-number">2.</span> <span class="nav-text">固定target-Q网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#剪切回报-值范围"><span class="nav-number">3.</span> <span class="nav-text">剪切回报/值范围</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DQN玩Atari游戏"><span class="nav-number">4.</span> <span class="nav-text">DQN玩Atari游戏</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-Double-DQN"><span class="nav-number">5.</span> <span class="nav-text">4. Double-DQN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-确定优先级的经验回放"><span class="nav-number">6.</span> <span class="nav-text">5.确定优先级的经验回放</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-决斗网络"><span class="nav-number">7.</span> <span class="nav-text">6. 决斗网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#加速训练"><span class="nav-number">8.</span> <span class="nav-text">加速训练</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#五-基于policy的强化学习"><span class="nav-number"></span> <span class="nav-text">五. 基于policy的强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Deep-Policy-Networks"><span class="nav-number"></span> <span class="nav-text">1. Deep Policy Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#策略方式"><span class="nav-number">1.</span> <span class="nav-text">策略方式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#目标函数-1"><span class="nav-number">2.</span> <span class="nav-text">目标函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Policy-Gradients"><span class="nav-number">3.</span> <span class="nav-text">Policy Gradients</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Actor-Critic算法"><span class="nav-number"></span> <span class="nav-text">2.  Actor-Critic算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Actor-网络"><span class="nav-number">1.</span> <span class="nav-text">Actor 网络</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Critic-网络"><span class="nav-number"></span> <span class="nav-text">Critic 网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Asynchronous-Advantage-Actor-Critic-A3C"><span class="nav-number"></span> <span class="nav-text">3.  Asynchronous Advantage Actor-Critic (A3C)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Actor-网络-1"><span class="nav-number">1.</span> <span class="nav-text">Actor 网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Critic-网络-1"><span class="nav-number">2.</span> <span class="nav-text">Critic 网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A3C用于Labyrinth游戏"><span class="nav-number">3.</span> <span class="nav-text">A3C用于Labyrinth游戏</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DRL如何处理高维连续action空间？"><span class="nav-number"></span> <span class="nav-text">DRL如何处理高维连续action空间？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Deep-DPG"><span class="nav-number"></span> <span class="nav-text">4. Deep DPG</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#经验回放-1"><span class="nav-number">1.</span> <span class="nav-text">经验回放</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Critic-网络-2"><span class="nav-number">2.</span> <span class="nav-text">Critic 网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Actor-网络-2"><span class="nav-number">3.</span> <span class="nav-text">Actor 网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DDPG用于Simulated-Physics"><span class="nav-number">4.</span> <span class="nav-text">DDPG用于Simulated Physics</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#六-基于model的强化学习"><span class="nav-number"></span> <span class="nav-text">六. 基于model的强化学习</span></a></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">susht</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  








  





  

  

  

  

  

  

</body>
</html>
