<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[cs234强化学习课程笔记-lecture4]]></title>
    <url>%2F2017%2F10%2F13%2Fcs234-lecture4%2F</url>
    <content type="text"><![CDATA[这段时间笔者在学习CS234强化学习课程，此文为Lecture1-Lecture4的总结博文。 本文结构： MDP Planning Basic RL with Lookup Tables Generalization in Model-free RL 一. MDP Planning在讲MDP Planing前先介绍一下MDP，马尔可夫决策过程。 Markov Decision Process马尔科夫决策过程，MDP可以表示成五元组: S表示一个有限的state集合，A表示一个有限的action集合，P是状态转移矩阵，在state s下执行action a 然后转移到状态s’的概率，γ是衰减因子，用来计算在state s后的未来总reward之和，也就是return。 MDP会有一个policy π，这里的Policy是指，在state s 下，执行的action的概率分布，这个叫随机性policy，比如在state s 下，现在有两个action，action a1和action a2，那么假设执行action a1的概率是0.8，那么执行action a2的概率就是0.2，我们可能会执行action a1，也可能会执行action a2。 但是在确定性的policy中，它会告诉你执行action a1或者执行action a2。 MDP还有两个价值函数，用来衡量当前的状况。Vπ（s）用来衡量state s的好坏程度，它是从当前state s开始，后面使用policy π去选择一系列action，得到的return的期望。 Qπ用来衡量在state s下执行action a的好坏程度，它是在当前state s下选择了action a，然后后面再去用policy π去选择后面的action，得到return的期望。 MDP PlanningMDP planning就是在已知MDP的情况下，去做planning。MDP planning跟强化学习都是序列决策问题，不同的地方在于，强化学习不知道外界的环境是怎么样的，MDP已经知道了环境是怎么样的。 举个打游戏的例子，强化学习就相当于，不告诉你游戏规则就直接让你玩，你只知道你可以执行哪些action，但是你不知道执行什么样的行为是好的，什么样的行为是不好的，你也不知道执行了某个action会跳到一个什么样的state，所以你会不断去尝试去猜测这个游戏应该怎么玩。MDP planning就相当于在游戏当中开挂了，知道了自己执行某个action后的状态会是什么样，再去优化自己的policy。这两者之间是有联系的，强化学习既要学习这个游戏的规则，又要学习怎么做planning。 在已知MDP的情况下，MDP planning要解决的问题有两种，第一种是prediction问题，给定一个policy，去评估一下这个policy怎么样，目标是算出在这个policy下，每个state的value function。第二种是control问题，这里也给出一个policy，但是它要去优化这个Policy，所以它的目标不仅是计算出最优的value function而且要给出最优的Policy。 在MDP Planning中，用Iterative Policy Evaluation可以解决prediction问题，用Policy Iteration和Value Iteration可以解决controll问题。 Bellman Equation贝尔曼方程，可以把一个问题划分成子问题求解，value function跟q function都可以表示成贝尔曼方程的形式，这里不再做贝尔曼方程的推导，只回顾贝尔曼方程的形式。 贝尔曼方程分成两种，一种是期望方程，一种是最优方程。在贝尔曼期望方程当中，Vπ(s)是在state s下，基于policy π的价值函数，假设从s可以转移到s’，那么V(s) 就是关于V(s’)的期望。V用来衡量当前state的好坏，Q用来衡量在当前state的情况下，选择某个action的好坏程度。 对应的，在贝尔曼最优方程当中，V*(s)是在state s下，遍历所有的action，选择最大的value作为最优的value，其实V*(s)也是关于V*(s’)的期望。根据V*可以求出Q*，选择Q*对应的的action作为π*。 Iterative Policy Evaluation把v用贝尔曼方程的形式表示出来后，我们就可以迭代去计算V函数。 迭代策略评估，要解决的问题是，评估一个policy，也就是基于当前的Policy，计算出每个状态的value function，目标是在已知policy下得到收敛的value function。迭代策略评估使用贝尔曼期望方程去更新value值，假设state s能到达state s’，那么第k+1次迭代中衡量state s的V，用第k次迭代中s’的V去更新。 举一个网格的例子，左上角和右下角是目的地，每走一步reward-1，这里面设置policy是random policy，我们现在要计算每个网格state对应的value K=0，初始化所有的state的取值v(s)为0 K=1，根据贝尔曼方程，上一次迭代的v（s‘）都是0，这一次算出来的v(s)都是-1 K=2，-1.7 = -1 + E(0,-1,-1)，理论上s’是属于整个集合s的，但是在这个例子里面-1.7的下一个状态s’实际上是只有三种可能的，所以在-1.7对应 的状态转移矩阵中，这三个状态s’的转移概率都是1/3，然后其它s’的转移概率都是0。-1.7右边，-1.7下面的两个-2.0，右-2.0 = -1 + E(-1, -1, -1)，下-2.0 = -1 + E(-1, -1, -1, -1)，虽然算出来都是-2.0，都是他们对应的状态转移矩阵是不一样，右边的-2.0可以转移到3个状态，下面的-2.0可以转移到4个状态。 k=无穷处，已经迭代到收敛，收敛之后这些value值都不变了，比如-18 = -1 + E(-14, -14, -20, -20) = -18。 Policy Iteration策略迭代解决的是control的问题，是在前面策略评估的基础上更新策略。策略迭代分为策略评估和策略提升两部分。 策略评估，基于当前的Policy计算出每个state的value function： 策略提升，基于刚刚算出来的v，对每个action再算一个Q ，选择使得Q最大的action，作为当前最优的Policy，也是下一步计算V的policy。 一开始给定一个policy，V也初始化为0，然后更新policy，用这个更新后的policy去重新评估v，再不断更新，最后会收敛要最优的V和policy。 还是举网格这个例子，我们刚刚是给定policy求解v值，现在要求最优的policy。需要注意的是，图中这个例子，在评估策略的时候依然使用固定的policy，这不是真正的策略迭代方法，真正的策略迭代在策略评估这一步需要用当前的policy。 这里左边计算v值的时候，使用的policy应该改成当前的policy，而不是random policy右边根据v值计算每个action对应的Q值，然后更新policy 现在考虑红圈圈住的那个state，第0次迭代的时候是random policy，第1次迭代的时候，根据左边的v去计算4个action所对应的Q值，这4个Q值是一样的，所以policy有4个箭头，第2次迭代的时候，算出来左边和上面的Q值是最大的并且是一样大的，所以policy有两个箭头。 一直迭代直到policy收敛，最终会得到最优的policy。在这个例子中，在k=3的时候poliy就已经收敛了，但是v还没有收敛，迭代到最后v和policy会一起收敛。如果更新策略评估中的policy，那么v和policy应该是连接收敛的。 Value Iteration策略迭代解决的是control的问题，其实值迭代解决的也是control的问题，虽然在迭代的过程中没有显式计算出policy，但是在得到最优的value值之后也就能得到最优的policy，这里选择使得v最大的action作为Policy。这里用的是贝尔曼最优方程，是不对应任何policy的。前面策略迭代用的是贝尔曼期望方程，对对应于某个policy的。 值迭代的最优化原理是当且仅当任何从s能达到的s′能在当前policy下获取最优的value，那么s也能在当前policy下获得最优value从这个原理出发，如果已知vs′的最优值，那么就能通过贝尔曼方程将v(s)的最优值推出来。因此从终点开始向起点推就能把全部状态最优值推出来。 现在对比一下这两种方法，走迷宫，我们要从起点走到终点，每走一步reward是-1。 在值迭代的方法中，每一个格子里面的数字表示这个state对应的value，我们要做的是求出这些value，再根据value去选择action，比如在-15这个位置，有3个action，一个是向上走，一个是向下走，现在求出来上面的Value值是-1+(-14)=-15，下面的value值是-17，那么我们就会选择往上走。这本身也是一种policy。 在策略迭代的方法中，直接算policy，每个格子里面的箭头是表示当前应该往哪个方向走。 二. Basic RL with Lookup Tables强化学习分为model based跟model free两种，强化学习是不知道环境的，上一节讲的mdp planning也是已经给出了状态转移矩阵和rewrad函数。Model based则是需要自己学习环境的状态转移矩阵跟reward，然后再去做planning。相当于environment + mdp planning。Model Free是指不需要去猜测环境的state和reward，直接跟环境去做交互，直接从环境中获取state和reward，我们这里只讨论model-free的方法。 前面的mdp planning通过动态规划的方式去解决已知环境的MDP问题，然后又根据是否更新policy将问题又划分成了prediction和control问题。 那么在未知环境下，也就是model free的情况下，同样需要解决prediction和control问题，这种新的问题只能与跟环境进行交互，得到一些经历去获取信息。 解决prediction的问题，是通过估算未知MDP的value function，解决control的问题，是最优化未知MDP的value function以及policy，所以上面这种又叫做value-based，下面这种叫policy-based 这一节讨论的是在未知环境中解决predict的问题，通过蒙特卡洛方法和时序差分学习，去评估policy。 Monte-Carlo Policy Evaluation蒙特卡洛策略评估，目标是基于某个Policy，从过往的经历中，去学习value函数。这里必须是完整的episodes，一个完整的episode指的是，从某个初始状态开始，一直执行到终止状态，这样一次遍历。 在讲MDP的时候讲到，每个state的value值是return的期望： 蒙特卡洛策略迭代方法，用多条经历的平均return，去代替return的期望。蒙特卡洛策略迭代又分成两种，First-Vist MC和Every-Vist MC。 这个图里面，给出了三条经历，每一条经历从state s0开始，执行一个action，会得到一个reward，reward就是蓝色的小点，然后会跳到下一个state，一直执行到终止状态。这里设置无衰减，R1(s)=1-2+0+1-3+5=2，R2(s)=-2+0+1-3+5=1。 假设我们现在要求状态s的value值，在第一条经历中，经过了两次s状态，在第三条经历中也经过了两次s状态。如果用first visit的方法，每一条经历只采第一个状态s，用这两个s去算return的均值。如果是every visit的方法，每一条经历的所有s状态，都要访问，就是说对这四个s算return的均值。 这些方法，有个问题就是，必须等到执行完所有episode，才能算value。假设有新来了一条经历，又要重新算。 Incremental Monte-Carlo针对这个问题，提出增量式的蒙特卡洛计算方法，当新来一个经历的时候，直接更新v，不用重新算。 这个公式我们之前推导过，其实和前面求均值是一样的，只是换了种写法。 为了计算更平稳，这里用α代替1/N: 我们现在解决了新经历这个问题，现在面临的另一个问题就是必须要走完一条经历的所有流程，得到return，才能计算value Temporal-Difference Learning时序差分学习解决了这个问题。 蒙特卡洛方法用真实的Gt值去更新value，既然我们现在不想走完整个流程再算value，那就预算return，时序差分学习用估算的return值去更新value。 TD(0)是指在某个state s下执行某个action后转移到下一个状态s′时，用去估算真实s的return, 再更新vs。 其中就叫做TD Target， 叫做TD Error 这是最简单的算法，我们也可以，先执行n个action转移到状态s’，再估算s的return，这个就叫Gt(n) 下面举个例子，来说明一下蒙特卡洛跟时序差分的区别。这个例子的policy是一直往左走。 现在我们考虑这么一个经历trajectory = (S3,TL,0,S2,TL,0,S2,TL,0,S1,TL,1,S1) 第一个问题：first visit MC算出来所有状态的value是多少？我们知道value就是return的均值，现在γ=1，那么所有时刻访问过的state的return都是1，所以对应的value也是1,所有state对应的value就是[1 1 1 0 0 0 0] 第二个问题：every vistit MC算出来的v(s2)是多少？也是1因为所有时刻访问的state的return都是1，return的均值也是1，所以s2对应的value是1 第三个问题：TD对state的评估结果是多少？我们先把这一条经历拆成四个元祖，TD是用Rt+1去更新Vst，只有第4个元祖的reward是1，也就是只有s1估算的return是1，那么s1的value值是α, 其它state的value值是0，所以所有state对应的value就是[α 0 0 0 0 0 0] 第四个问题：前面是只更新了一次，假设更新很多次，那TD和MC又怎么更新？TD会不断从过去的元祖里面重复采样更新，MC不断从过去的经历中重复采样更新。 在这个例子，我们只有一条经历，现在对这条经历重复采样，分别求MC跟TD的结果。 首先，蒙特卡洛的结果不变，因为MC对某个state求value的时候，是对这个state的return求均值，重复采样，相当于对多条相同的经历，求return的均值，均值是不变的，所以在first-visit中所有state对应的value依然是[1 1 1 0 0 0 0] 在TD中，在这条经历的基础上，假设又采样出这些下面这些元祖，现在求TD的结果。最初在这个经历后，我们算出来s1的value值是a，其它state是0。然后用下面第一个元祖更新，reward是0，所以value都不变，第二个元祖，r=1，更新v(s1)=2a, 3a, 4a，则最后所有state对应的value就是[4α 0 0 0 0 0 0] 最后一个问题，在TD中，这些元祖采样的顺序，会不会影响到算法的收敛？这个肯定会影响的，因为我们是根据v（st+1）更新v(St)，采样顺序不同，v(st+1)就会不一样。那么，什么样的采样顺序才是一个好的顺序呢？应该先采那些临近后面state的样本。 MC方法会收敛到最小化均方误差的解，而TD方法会收敛到MDP问题的解，会找出状态转移概率和reward函数，还是在解决MDP问题。 Policy Evaluation下面总结一下用Policy Evaluation解决强化学习问题的一些算法。 最左边的竖线表示有没有考虑所有的可能发生的情况，如果考虑了所有的可能发生的情况那么就是动态规划，如果是部分采样就是时序差分。 下面的横线表示如果在一次经历中，考虑了全部的动作就是Monte-Carlo，如果只考虑部分动作就是时序差分。如果又考虑全部情况又考虑每一种情况的全部动作就是穷举。 1)Dynamic Programming Backup:这个图表示动态规划考虑了所有可能发生的情况。白色的点表示state，黑色的点表示action，在state st下，有两个action可以选择，选择某一个action之后，会得到一个reward rt+1，到达状态st+1，绿色的T表示这一条经历结束了。求解v(st)需要回溯到所有的st+1. 2)时序差分，求解v(st)需要回溯到，在t时刻选择某个action后到达的st+1 3)蒙特卡洛，求解v(st)需要回溯到一整条路径。 三. Generalization in Model-free RL在现实生活中，状态一般是很多的，比如说大型画面游戏，每一帧就是一个state，或者是那种不连续的状态，比如状态是0.1, 0.01这种。 针对这种大规模的状态，如果算出每种状态下的真实value function，没有足够的内存也没有足够的计算能力，此外考虑到比较接近的状态它们的值函数取值应该是很相似的，这是一种泛化能力。 所以需要一个算法去近似价值函数，可以用线性特征组合，神经网络等等。这里我们只讨论线性的方法去逼近价值函数。 Linear Value Function Approximation (VFA)线性特征组合，把state表示成特征向量，对这些特征向量进行线性组合，算出来这个值就是这个state所对应的value。 定义目标函数是真实v值和算出来v值的平方差，在所有policy下的期望： 用梯度下降法去更新参数： 如果把价值函数近似用在蒙特卡洛方法上，目标就是去近似Gt，这里面Gt是对真实V值无偏差的估计，所以这属于有监督学习，监督信息就是Gt。蒙特卡洛方法会收敛到局部最优解 在时序差分中，目标是近似TD target，这个return是对真实v值有偏差的采样，就是说它实际上并不等于真实v值，只是一个估算值。同样是有监督学习。TD会收敛到全局最优。]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Deep_RL_Tutorial》阅读笔记]]></title>
    <url>%2F2017%2F09%2F23%2FDeep-RL-Tutorial%2F</url>
    <content type="text"><![CDATA[本文翻译自David Silver的《Tutorial: Deep Reinforcement Learning》，仅为个人笔记，如有写错，请联系susht3@foxmail.com，一起讨论。 本文结构分为以下几部分： 一. 前言 二. 深度学习简介 三. 强化学习简介 四. 基于value的强化学习 五. 基于policy的强化学习 六. 基于model的强化学习 一. 前言深度学习 vs 强化学习深度学习是特征学习，给定一个目标，学习目标的特征，使用最原始的特征输入，利用最少的领域知识。 强化学习是决策学习，针对具有选择action能力的agent，每个action都会影响到agent未来的状态state，利用一个标量的奖励reward来衡量成功度，目标是选择一系列的action以最大化未来的奖励reward。 深度强化学习结合DL+RL，RL定义目标，DL给出计算过程。 二. 深度学习简介深度学习是用来实现机器学习的一种方法，深度神经网络用于理解深度表征。 1. 深度表征深度表征由很多函数组成： 通过后向传播来更新梯度： 注意：网络过深会导致网络权重更新不稳定，会带来梯度爆炸或者梯度消失的问题，有时候需要进行梯度截断。梯度消失，层数越多，求导结果越小，所以越是靠前的网络层权重几乎不更新，只学习到接近最后输出层的几层网络。梯度爆炸，w比较大。还跟学习率有关。梯度爆炸和梯度消失的本质原因：梯度反向传播中的连乘效应。 2. 深度神经网络线性变换： 非线性激活函数： loss函数： 有多种loss函数，平方差，对数似然函数，交叉熵等等。 3. 随机梯度下降法对loss求导： 根据梯度下降的方向去调整w： 梯度下降法用于求解一个函数J(θ)的最大值或者最小值，而一个函数沿着梯度下降的方向是下降最快的，导数为0的那一点就是最值点。现在目标是更新θ使得J(θ)最小，先选个初始值，然后不断往梯度的方向更新θ，直到θ的值不再改变。更新公式中学习率决定下降步伐，梯度决定下降方向。学习率用于控制迭代速度，取值太小会导致迭代过慢，取值太大可能错过最值点。 下山例子 这里纵坐标表示目函数的值（即 J( θ) 的值），两个横坐标分别表示参数 θ1 和 θ2。从初始点开总是往下降速度最快的地方走 ，这个地方也就是该点的梯度。 二维的等高线图 迭代特点：局部最优解初始值有影响，初始值不同，得到的最小值也有不同；每一步都选择梯度下降的方向走，越是接近最小值，下降速度就越慢。 批量梯度下降：每次更新都遍历m个训练样本。缺点是训练集很大的时候，耗时太长。 随机梯度下降：依次使用第i个样本进行更新。缺点是它可能在全局最优解的附近徘徊，但是总体趋向全局最小值。 4. 权重共享这是减少参数数量的关键！ 循环神经网络 RNN在时间步长上共享参数 卷积神经网络 CNN在空间区域中共享参数。 局部连接+权重共享。局部连接，每个隐藏层节点每次都只连接到一个局部区域。权重共享，卷积核的权重参数共享到所有的神经元。 三. 强化学习简介1. RL的地位无处不在的RL，作为许多领域的中心思想，优化决策以获得最佳结果。RL是一个序列决策问题，选择一系列的决策，使得最终收益最高。 （下面这段话引用自chenrudan） 强化学习是多学科多领域交叉的一个产物，它的本质就是解决“decision making”问题，即学会自动进行决策。在computer science领域体现为机器学习算法。在Engineering领域体现在决定the sequence of actions来得到最好的结果。在Neuroscience领域体现在理解人类大脑如何做出决策，主要的研究是reward system。在Psychology领域，研究动物如何做出决策，动物的行为是由什么导致的。在Economics领域体现在博弈论的研究。 2. 基本概念Agent 和 Environment 在每个时刻下： Agent:执行某个action at，然后接收来自环境的观察值ot与瞬时奖励rt Environment:接收来自agent 的一个动作action，发出观察信号o(t+1)，和奖励信号r(t+1) (confuse：这里我觉得应该是ot和rt，也有可能是因为延时？) State Experience(历史状态):包含一系列的从第一个时刻开始的observations, actions, rewards State(状态):state是experience的总和。在某个时刻t下，状态state st是这个时刻以及之前所有时刻的所有历史状态的函数。其实严格意义上有两种state：environment state是环境自身的状态，agent state是环境反馈给agent的状态，也就是平常简称的state。环境会通过observation来告诉agent当前的环境状态。 观测性:在一个全观测的环境中，agent state等同于environment state。在某个时刻t下，state是当前时刻下观测值ot的一个函数，这里具备马尔科夫性质。在一个部分可观测的环境中，那么agent获取到的就不是最直接的environment state。 3. Agent的组成一个Agent由policy,value function和model这三部分组成，但是这三部分不是必须要存在的。 Policypolicy是agent的决策行为，根据state来选择action，函数形式是将state映射为action，包含两种表达方式。确定性策略根据state直接选择某个action。随机性策略根据state生成action的概率分布，再从中选择action。 Value functionvalue函数预测当前state s下，未来可能获得的reward的期望，用来衡量当前state的好坏。Vπ(s)=Eπ[Rt+1+rRt+2+…|St=s]，已有policy去选择action。而Q函数是奖励总值的期望，给定state s，action a，policy π，以及衰减因子γ，计算综合奖励的期望。Q用来衡量当前state下选择某个action的好坏。 注意！V和Q的区别在于，v是state value，Q是state-action value。Q里面的action a，不是用这个policy去算当前的action，而是当前state s已经采取了这个action a，后面的state s’再利用这个policy去选择后面的action a’，得到后面的reward。 如何更新value： v可以采取某个policy选择一个action，或者遍历所有action选择v最大的。公式左边是即时奖励，右边是带有衰减因子的未来奖励。 贝尔曼方程Q函数还可以分解为贝尔曼方程： 后面的r用Q(s’)来表示，s’为当前state s的下一个状态，a’为下一个actionQ函数是一个期望！关于后面state和action的一个期望。 优化价值函数就是使得价值函数达到最大值。给定state s和action a，计算在所有policy下，当前state和action的Q函数，而最大的Q就是当前state和action下最优的Q函数。也就是说，在当前state下，每个action都有一个最优的Q函数。然后遍历所有action，在这些最优Q当中，再次选择一个最大Q，选择这个最大Q的action，以此作为当前state下的policy π。 注意：最优Q是带有星号的Q，Q其实就是在Q中挑选出最优的那个。 最优价值函数也可以分解为贝尔曼方程： Q和Q的贝尔曼方程区别在于，Q是对s’和a’的期望，Q是对s’的期望，因为Q*会选择使得Q(s’，a’)最大的action a’ ModelModel从经历中学习，学习环境的工作方式，预测下一个state或者下一次reward。 4. 实现强化学习的方法前面提到agent的三个组成部分，可以根据这三部分去实现强化学习。 基于value的方法估算Q函数的最大值Q*(s, a)，注意：这是基于任意policy下都能得到的最优Q值！policy π的参数会变，所以会有不同的π，那么不同的π就会得到不同的action，其实 Under any policy 最后就是会遍历到所有的action。 基于policy的方法寻找最优policy π*，在这个policy下能够得到最大的未来奖励。 基于model的方法对环境进行建模，用这个模型来进行规划，比如前向搜索。 DeepMind-DRL例子游戏：Atari 游戏、扑克、围棋 探索世界：3D 世界、迷宫 控制物理系统：操作、步行、游泳 与用户互动：推荐、优化、个性化 实现方法：使用深度学习去模拟Q，policy，model，再用随机梯度下降法去优化loss函数。 四. 基于value的深度学习1. Q网络用神经网络对Q进行建模： 第一类做法： 输入s与a，输出一个Q(s，a，w)值，其中w为网络权重 第二类做法： 输入s，输出一个Q(s，ai，w)的向量，维度为action的维度大小 2. Q-Learning算法目标函数把最优的Q*定义为我们要求的目标，用随机梯度下降法去最小化均方差MSE 神经网络所产生的问题采样之间的相关性：当前的state和action和上一步执行的action非常接近！数据集具备序列性质，具有很强的相关性。 目标的不稳定性：注意 我们学习的目标是依赖于目标本身的。不平稳性会导致近似函数螺旋式失控，policy会有震荡。 为了解决这些问题，接下来会介绍DQN算法。 3. DQN算法经验回放从过往的经历构建出一个数据集(s，a，r’，s’)，从这个数据集中随机采样一些经历(s，a，r，s’)进行更新。 这些采样出来的经历是独立分布的，这样就可以打破训练集之间的相关性。 注意！构建数据集的时候使用(s，a，r’，s’)，但是采样的时候使用(s，a，r，s’)。为啥？还是因为 延迟性？？mark 固定target-Q网络公式左边是target Q，右边是我们训练的Q网络。使用MSE损失函数。 这里暂时hold住Q网络以前的权重w-，用这个固定住的参数w-去算target Q，然后周期性更新W-为w。而Q网络仍是你Q网络，Q网络是时刻更新的，只是target Q会暂时固定住的。这样可以减少震荡。 剪切回报/值范围设置reward在[-1，+1]范围内，可以保证Q的值不会太大，好求梯度。 DQN玩Atari游戏从像素s中端到端学习Q（s，a），输入是前面4帧的像素，作为state，输出是对应按钮位置的Q（s，a），那么reward就是执行这个action后，得分和从前的差值了。 接下来会介绍一些基于DQN的改进方法。 4. Double-DQN用两个网络，Q(w-)用于选择action，Q(w)用于评估action Double-DQN与DQN的区别在于，在下一个state s’中，DQN遍历a’使用max Q作为target，这里评估action的Q是暂时固定住的Q，这是会有偏差的。Double-DQN同样也是先选出max Q对应的action a’，然后再用网络Q(w-)去评估这个action，以Q(a’，w-)的结果作为target Q。 5.确定优先级的经验回放从前做经验回放的时候，所有经历都是等权重的。这里以DQN的误差去衡量经历的优先级，越是做得差的经历越是要回放。 6. 决斗网络将Q网络分成两个channel，Action-indepent用于计算当忽略了某些动作时 你会得到多少奖励。在V(s，v)中，小v是V网络的参数，s是state。Action-dependent用于计算当你采取了某个动作a时 你会得到多少好处。最后结合这两部分，总和作为Q。 加速训练Gorila结构，使用不同机器，并行化运行Q网络。 如果你没有google的资源，还可以使用异步强化学习：利用标准 CPU 的多线程；并行执行agent的多个实例；线程间共享网络参数；并行消除数据相关性；替代经历回放；在单个机器上模仿Gorila的加速过程。 五. 基于policy的强化学习1. Deep Policy Networks策略方式随机性policy：根据state生成action的概率分布，表示在某个state下执行某个action的概率。随机性policy比确定性policy要好。 确定性policy：根据state直接生成action 目标函数将目标函数定义为 总reward，使用随机梯度下降法来最大化目标函数。 这个写法感觉有点怪怪的，其中u是网络的参数，π是policy，点号不知道是什么。可以写成，在这个policy下，所有action对应的的总reward的期望。目标形式依然是优化Q，但是和前面不同的是，这里Q的参数是policy的参数。 Policy Gradients目标函数分别对随机性policy和确定性policy求梯度： 对于随机性policy，把L(u)写成E的形式再求导，就可以得到图中结果。公式中第一项是score function，用来衡量policy选择这个action的程度，第二项是Q，用来衡量这个action的好坏。如果这个action表现较好，我们就调整policy趋近这个action，最终拉近这两个分布。 对于确定性policy，链式求导。 2. Actor-Critic算法Actor-Critic算法包含两个网络，Actor 网络用于选择action，Critic 网络用于评估这个action的好坏程度，计算value function，去指导Actor网络更新梯度。Actor基于policy gradients，可以在连续的actio空间选择一个action，而critic基于Q-learning，可以进行单步更新，注意policy gradient是回合更新，会降低学习效率。所以Actor-Critic是结合了基于policy和基于value的方法。 Actor 网络Actor网络用于学习policy，输入state，输出action Critic 网络Critic网络用于学习Q，以评估action的好坏，输入state和action，输出对应的Q，Critic的目标就是估计这个policy的贝尔曼方程。 那么接下来actor就可以用这个Q去更新自己的梯度，往提高Q的方向去更新。 这里w是critic的参数，u是actor的参数，这里是对u求导，以更新policy。 3. Asynchronous Advantage Actor-Critic (A3C)V是state-value function，用于评估在当前state的好坏。Q则是state-action value，这里和之前定义的Q所有不同，这里只有前n个reward是用reward来算，后面的reward总值使用V去评估。 Actor 网络与先前actor不同的地方是，公式右边变成Q-V，这个就叫Advantage，表示采取这个action比平均state下要好出多少。 Critic 网络Critic网络评估的是V值，目标函数是 根据q 和V 的MSE函数。 咩咩咩？？？mark A3C用于Labyrinth游戏定义state为当前画面的像素，使用lstm对state进行建模，每个state输出action的概率分布，以及这个state的value值。 DRL如何处理高维连续action空间？ 如果对Q算一个max，计算量会非常大。 Actor-Critic算法不需要算max Q，避开了这个问题。 Q值是可微的。 确定性的policy gradient会对Q求导。 4. Deep DPGDDPG同时借鉴了DQN和Actor-Critic，DDPG的优势在于连续action空间。 经验回放前面提到的。 Critic 网络DDPG和DQN不一样的地方在于，DDPG多了个policy π，π的参数为u-，target-Q中会暂时hold住u-和w- Actor 网络Actor网络更新的时候也是朝着提高Q的方向。 DDPG用于Simulated Physics 六. 基于model的强化学习未完待续 暂时写不下去了….回宿舍]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
</search>
